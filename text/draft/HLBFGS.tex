Программа работает на методе HLBFGS 
(Hybrid Limited Memory Broyden-Fletcher-Goldfarb-Shanno Method)
http://research.microsoft.com/en-us/um/people/yangliu/software/hlbfgs/
http://ru.wikipedia.org/wiki/Алгоритм_Бройдена_--_Флетчера_--_Гольдфарба_--_Шано
http://en.wikipedia.org/wiki/Limited-memory_BFGS
http://crsouza.blogspot.ru/2012/02/limited-memory-broydenfletchergoldfarbs.html

Алгоритм Бройдена-Флетчера-Гольдфарба-Шанно -- итерационный метод численной 
оптимизации, предназначенный для нахождения локального максимума/минимума 
нелинейного функционала без ограничений. 

BFGS -- один из наиболее широко применяемых квазиньютоновских методов. В 
квазиньютоновских методах не вычисляется напрямую гессиан функции. Вместо этого 
гессиан оценивается приближенно, исходя из сделанных до этого шагов. Также 
существуют модификации данного метода с ограниченным использованием памяти 
(L-BFGS), который предназначен для решения нелинейных задач с большим 
количеством неизвестных, а также модификация с ограниченным использованием 
памяти в многомерном кубе (L-BFGS-B).

Описание
Пусть решается задача оптимизации функционала:
\[
    arg \min\limits_{x} f(x)
\]

Методы второго порядка решают данную задачу итерационно, с помощью разложения 
функции в полином второй степени:
\[
    f(x_k+p) = f(x_k) + \nabla f^{T}(x_k)p + \frac{1}{2}p^{T}H(x_k)p
\]
где \( H \) -- гессиан функционала \( f \) в точке \( x \). Зачастую вычисление 
гессиана трудоёмки, поэтому BFGS алгоритм вместо настоящего значения 
\( H(x) \) вычисляет приближенное значение \( B_k \), после чего находит 
минимум полученной квадратичной задачи:
\[
    p_k = -B_{k}^{-1}\nabla f(x_k)
\]

Как правило, после этого осуществляется поиск вдоль данного направления точки, 
для которой выполняются условия Вольфе.
<< http://ru.wikipedia.org/wiki/Условия_Вольфе >>

В качестве начального приближения гессиана можно брать любую невырожденную, 
хорошо обусловленную матрицу. Часто берут единичную матрицу. Приближенное 
значение гессиана на следующем шаге вычисляется по формуле:
\[
    B_{k+1} = B_k - \frac{B_k s_k s_{k}^{T} B_k}{s_{k}^{T} B_k s_k} + 
        \frac{y_k y_{k}^{T}}{y_{k}^{T} s_k}
\]
где \( s_k = x_{k+1} - x_l \) -- шаг алгоритма на итерации, 
\( y_k = \nabla f_{k+1} - \nabla f_k \) -- изменение градиента на итерации.

Поскольку вычисление обратной матрицы вычислительно сложно, вместо того, чтобы 
вычислять \( B_k \), обновляется обратная \( B_k \) матрица 
\( C_k = B_{k}^{-1} \)
\[
    C_{k+1} = \left( I - \rho_k s_k y_{k}^{T} \right)
        C_k \left( I - \rho_k y_k s_{k}^{T} \right) + \rho_k s_k s_{k}^{T}
\]

---
D. C. Liu and J. Nocedal, On the Limited Memory Method for Large Scale 
Optimization, Mathematical Programming B, 45, 3, pp. 503-528, 1999.

\section{Limited-Memory BFGS Method}

The limited-memory BFGS method belongs to a class of methods called 
limited-memory quasi-Newton methods. This class of methods is especially useful 
in solvin large scale problems with Hessian matrices that are too expensive to 
compute or too dense to store and manipulate easily. The major advantage is 
that these methods don't requre the storage of the \( N\times N \) full 
Hessian matrix, where N is the number of variables; instead they only store 
2m vectors of length N. Usually, these vectors are applied to update a simple 
initial matrix, such as a multiple of the identity, to form an approximation 
to the current Hessian. The same idea can be applied to inverse of the Hessian. 
Suppose at step \( k \), we have stored m most recent correction pairs 
\( (s_i,y_i) \), \( i = k-m, \ldots, k-1 \), where
\begin{gather}
    s_i = x_{i+1} - x_i \\
    y_i = \nabla f(x_{i+1}) - \nabla f(x_i)
\end{gather}

We can construct an approximation to the inverse of \( \nabla^2 f_k \) by first 
selecting an initial inverse Hessian approximation \( H_{k}^{0} \), and then 
applying the update pairs to it:
\begin{gather}
    H_{k}^{1} = update(H_{k}^{0}, s_{k-m}, y_{k-m}) \label{eq:3}\\
    \hdots \\
    H_{k}^{j+1} = update(H_{k}^{j}, s_{k-m+j}, y_{k-m+j})
\end{gather}

Finally we set \( H_k = H_{k}^{m} \), and use \( H_k \) to find the new 
direction \( p_k \):
\begin{equation}
    p_k = -H_{k}*\nabla f(x_k)
\end{equation}

The next iteration is then given by
\begin{equation}
    x_{k+1} = x_{k} + \alpha p_k
\end{equation}

where \( \alpha \) is chosen by a line search method.

This approach works with a variety of update formulas. The most widely used in 
this context is the BFGS inverse updating formula, given by:
\begin{equation}
    update(H,s,y) = V^T HV + pss^T
\end{equation}

where
\begin{equation}
    \rho = \frac{1}{y^T s}, \quad V = I - \rho ys^T
\end{equation}

The matrix \( H^{k}_{0} \) in \eqref{eq:3} should be a rough approximation to 
the inverse of the real Hessian, and usually is given by:
\begin{equation}
     H^{0}_{k} = \frac{s^{T}_{k-1} y_{k-1}}{y^{T}_{k-1} y_{k-1}}I 
\end{equation}

Previous studies have shown that \( 3 \leq m \leq 7 \) is a reasonable choice, 
and increasing m doesn't necessarily imply improved performance. The main 
weakness of the L-BFGS method is that it may converge very slowly in terms of 
number of iterations for ill-conditioned problems, which will require a large 
number of potentially costly function evaluations to be perfomed. Our goal 
is to find better choices of \( H^{0}_{k} \) that improve the speed of the 
method.

\section{Preconditioned L-BFGS Method}
The performace of the L-BFGS method relies upon having a good approximation 
to the actual Hessian. The correction pairs are used to correct the behavior 
of \( H^{0}_{k} \). So if we start with a better \( H^{0}_{k} \), we should 
expect a better approximation to the actual \( \nabla^2 f^{-1}(x_k) \), and 
hopefully, a more quickly convergent L-BFGS method.

...